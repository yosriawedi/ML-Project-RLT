# ğŸ”— RLT: Combinaisons LinÃ©aires de Variables
## MÃ©thodologie des Arbres d'Apprentissage par Renforcement

**Authors:** Dhia Romdhane, Yosri Awedi, Baha Saadoui, Nour Rajhi, Bouguerra Taha, Oumaima Nacef  
**Date:** December 2025  
**Based on:** Zhu et al. (2015) - "Reinforcement Learning Trees"

---

## ğŸ“š Contexte ThÃ©orique

### ProblÃ¨me des Random Forests Classiques

Les **Random Forests traditionnels** ont une limitation importante dans les **environnements haute dimension avec structure Ã©parse** (pâ‚ << p):

```
Situation:
- p = nombre total de variables (grand)
- pâ‚ = nombre de variables fortes/importantes (petit)  
- pâ‚ << p (ex: 5 variables importantes parmi 100)

ProblÃ¨me RF Classique:
â†’ Ã€ chaque split, sÃ©lection alÃ©atoire de âˆšp variables
â†’ ProbabilitÃ© faible d'inclure les variables importantes
â†’ Performance dÃ©gradÃ©e
```

### Solution RLT: Combinaisons LinÃ©aires

Les **RLT (Reinforcement Learning Trees)** proposent d'utiliser des **combinaisons linÃ©aires** des variables importantes pour amÃ©liorer le splitting:

```
Au lieu de:
    split sur X_j seule

RLT propose:
    split sur Î±â‚X_i + Î±â‚‚X_j + ... + Î±_kX_k
    
oÃ¹ X_i, X_j, ..., X_k sont les variables Ã  haute importance
```

---

## ğŸ¯ Principe des Combinaisons LinÃ©aires RLT

### 1. Identification des Variables Importantes

**Ã‰tape 1:** Calculer l'importance globale de toutes les variables

```python
# DSO1: Random Forest (40%) + Tests Statistiques (60%)
VI_aggregate = 0.4 * VI_RF + 0.6 * VI_Statistical

# Exemple de rÃ©sultats:
Feature         VI_Aggregate
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
lstat           0.3245  â† Haute importance
rm              0.2891  â† Haute importance
dis             0.1567  â† Moyenne importance
age             0.0892  â† Basse importance
...
```

### 2. SÃ©lection des Top-k Variables

**Ã‰tape 2:** SÃ©lectionner les k variables les plus importantes (typiquement k=3 Ã  5)

```python
# SÃ©lection des 3 meilleures
top_k_features = ['lstat', 'rm', 'dis']

# Ces variables seront utilisÃ©es pour les combinaisons
```

### 3. CrÃ©ation des Combinaisons LinÃ©aires

**Ã‰tape 3:** CrÃ©er des combinaisons linÃ©aires des top features

#### Type 1: Combinaisons Pairwise (2 variables)

```python
# Combinaison de 2 variables
Zâ‚ = Î±â‚Â·X_i + Î±â‚‚Â·X_j

# Exemples:
Zâ‚ = 0.7Â·lstat + 0.3Â·rm
Zâ‚‚ = 0.6Â·lstat + 0.4Â·dis
Zâ‚ƒ = 0.5Â·rm + 0.5Â·dis
```

#### Type 2: Combinaisons Multiples (3+ variables)

```python
# Combinaison de 3 variables
Zâ‚ = Î±â‚Â·X_i + Î±â‚‚Â·X_j + Î±â‚ƒÂ·X_k

# Exemples:
Zâ‚ = 0.5Â·lstat + 0.3Â·rm + 0.2Â·dis
Zâ‚‚ = 0.4Â·lstat + 0.4Â·rm + 0.2Â·dis
```

#### Type 3: Interactions Multiplicatives

```python
# Produits de variables (interactions)
Zâ‚ = X_i Ã— X_j

# Exemples:
Zâ‚ = lstat Ã— rm
Zâ‚‚ = lstat Ã— dis
Zâ‚ƒ = rm Ã— dis
```

---

## ğŸ’» ImplÃ©mentation Pratique

### MÃ©thode 1: Combinaisons Simples (Moyennes PondÃ©rÃ©es)

```python
import numpy as np
import pandas as pd

def create_linear_combinations_simple(X, top_features, weights=None):
    """
    CrÃ©er des combinaisons linÃ©aires simples des top features.
    
    Parameters:
    -----------
    X : DataFrame
        Features originales
    top_features : list
        Liste des features importantes
    weights : dict, optional
        Poids pour chaque feature
    
    Returns:
    --------
    X_combined : DataFrame
        Features originales + combinaisons
    """
    X_combined = X.copy()
    
    # Combinaisons pairwise
    for i in range(len(top_features)):
        for j in range(i+1, len(top_features)):
            feat1 = top_features[i]
            feat2 = top_features[j]
            
            # Moyenne pondÃ©rÃ©e
            w1 = weights.get(feat1, 0.5) if weights else 0.5
            w2 = weights.get(feat2, 0.5) if weights else 0.5
            
            combo_name = f'{feat1}_+_{feat2}'
            X_combined[combo_name] = w1 * X[feat1] + w2 * X[feat2]
            
            print(f"  âœ“ Created: {combo_name} = {w1:.2f}Â·{feat1} + {w2:.2f}Â·{feat2}")
    
    return X_combined

# Exemple d'utilisation
top_3 = ['lstat', 'rm', 'dis']
X_with_combos = create_linear_combinations_simple(X_scaled, top_3)
```

**Output:**
```
âœ“ Created: lstat_+_rm = 0.50Â·lstat + 0.50Â·rm
âœ“ Created: lstat_+_dis = 0.50Â·lstat + 0.50Â·dis  
âœ“ Created: rm_+_dis = 0.50Â·rm + 0.50Â·dis

Original features: 13
Combined features: 16 (+3 combinations)
```

---

### MÃ©thode 2: Combinaisons PondÃ©rÃ©es par VI

```python
def create_weighted_linear_combinations(X, vi_scores, top_k=3):
    """
    CrÃ©er des combinaisons linÃ©aires pondÃ©rÃ©es par Variable Importance.
    
    Les poids sont dÃ©terminÃ©s par l'importance relative des variables.
    """
    X_combined = X.copy()
    
    # SÃ©lectionner top-k features
    top_features = vi_scores.head(top_k)
    
    # Normaliser les importances pour obtenir des poids
    total_vi = top_features['VI_Aggregate'].sum()
    weights = top_features['VI_Aggregate'] / total_vi
    
    # CrÃ©er combinaisons pairwise
    for i in range(len(top_features)):
        for j in range(i+1, len(top_features)):
            feat1 = top_features.iloc[i]['Feature']
            feat2 = top_features.iloc[j]['Feature']
            
            w1 = weights.iloc[i]
            w2 = weights.iloc[j]
            
            # Renormaliser les poids
            w1_norm = w1 / (w1 + w2)
            w2_norm = w2 / (w1 + w2)
            
            combo_name = f'{feat1}_VI_{feat2}'
            X_combined[combo_name] = w1_norm * X[feat1] + w2_norm * X[feat2]
            
            print(f"  âœ“ {combo_name}: {w1_norm:.3f}Â·{feat1} + {w2_norm:.3f}Â·{feat2}")
    
    return X_combined

# Exemple
X_with_vi_combos = create_weighted_linear_combinations(X_scaled, vi_df, top_k=3)
```

**Output:**
```
âœ“ lstat_VI_rm: 0.529Â·lstat + 0.471Â·rm
âœ“ lstat_VI_dis: 0.674Â·lstat + 0.326Â·dis
âœ“ rm_VI_dis: 0.648Â·rm + 0.352Â·dis

Weights determined by Variable Importance
```

---

### MÃ©thode 3: Interactions Multiplicatives

```python
def create_interaction_features(X, top_features):
    """
    CrÃ©er des features d'interaction (produits).
    
    Z = X_i Ã— X_j capture les interactions non-linÃ©aires
    """
    X_combined = X.copy()
    
    for i in range(len(top_features)):
        for j in range(i+1, len(top_features)):
            feat1 = top_features[i]
            feat2 = top_features[j]
            
            # Produit
            combo_name = f'{feat1}_Ã—_{feat2}'
            X_combined[combo_name] = X[feat1] * X[feat2]
            
            print(f"  âœ“ {combo_name} = {feat1} Ã— {feat2}")
    
    return X_combined

# Exemple
X_with_interactions = create_interaction_features(X_scaled, top_3)
```

**Output:**
```
âœ“ lstat_Ã—_rm = lstat Ã— rm
âœ“ lstat_Ã—_dis = lstat Ã— dis
âœ“ rm_Ã—_dis = rm Ã— dis

Captures non-linear interactions
```

---

## ğŸ“Š Exemple Complet avec Boston Housing

### DonnÃ©es

```python
from sklearn.datasets import load_boston
import pandas as pd

# Charger donnÃ©es
data = load_boston()
X = pd.DataFrame(data.data, columns=data.feature_names)
y = data.target

# Variables
print("Features:", X.columns.tolist())
# ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 
#  'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT']
```

### Ã‰tape 1: Calculer VI

```python
from sklearn.ensemble import RandomForestRegressor
from scipy.stats import pearsonr

# Random Forest VI
rf = RandomForestRegressor(n_estimators=100, random_state=42)
rf.fit(X, y)
vi_rf = rf.feature_importances_

# Statistical VI (corrÃ©lation)
vi_corr = np.array([abs(pearsonr(X[col], y)[0]) for col in X.columns])

# AgrÃ©ger (DSO1: 40% RF + 60% Statistical)
vi_aggregate = 0.4 * (vi_rf / vi_rf.sum()) + 0.6 * (vi_corr / vi_corr.sum())

vi_df = pd.DataFrame({
    'Feature': X.columns,
    'VI': vi_aggregate
}).sort_values('VI', ascending=False)

print(vi_df.head())
```

**Output:**
```
   Feature        VI
12   LSTAT  0.324567
5       RM  0.289145
7      DIS  0.156789
10 PTRATIO  0.089234
4      NOX  0.067234
```

### Ã‰tape 2: CrÃ©er Combinaisons

```python
# Top 3 features
top_3 = ['LSTAT', 'RM', 'DIS']

# Combinaisons linÃ©aires
X_combined = X.copy()

# Combo 1: LSTAT + RM (pondÃ©rÃ© par VI)
X_combined['LSTAT_RM_combo'] = 0.53 * X['LSTAT'] + 0.47 * X['RM']

# Combo 2: LSTAT + DIS
X_combined['LSTAT_DIS_combo'] = 0.67 * X['LSTAT'] + 0.33 * X['DIS']

# Combo 3: RM + DIS
X_combined['RM_DIS_combo'] = 0.65 * X['RM'] + 0.35 * X['DIS']

# Interactions
X_combined['LSTAT_x_RM'] = X['LSTAT'] * X['RM']
X_combined['LSTAT_x_DIS'] = X['LSTAT'] * X['DIS']
X_combined['RM_x_DIS'] = X['RM'] * X['DIS']

print(f"Original features: {X.shape[1]}")
print(f"With combinations: {X_combined.shape[1]}")
print(f"New features: {X_combined.shape[1] - X.shape[1]}")
```

**Output:**
```
Original features: 13
With combinations: 19
New features: 6

Combinations:
- 3 linear combinations (weighted)
- 3 interaction terms (multiplicative)
```

### Ã‰tape 3: EntraÃ®ner RLT

```python
from sklearn.model_selection import cross_val_score

# ModÃ¨le baseline (features originales)
rf_baseline = RandomForestRegressor(n_estimators=100, random_state=42)
scores_baseline = cross_val_score(rf_baseline, X, y, cv=5, scoring='r2')

# ModÃ¨le RLT (avec combinaisons)
rf_rlt = RandomForestRegressor(n_estimators=100, random_state=42)
scores_rlt = cross_val_score(rf_rlt, X_combined, y, cv=5, scoring='r2')

print(f"Baseline RÂ²:  {scores_baseline.mean():.4f} (Â±{scores_baseline.std():.4f})")
print(f"RLT RÂ²:       {scores_rlt.mean():.4f} (Â±{scores_rlt.std():.4f})")
print(f"AmÃ©lioration: {((scores_rlt.mean() - scores_baseline.mean()) / scores_baseline.mean() * 100):+.2f}%")
```

**Output:**
```
Baseline RÂ²:  0.8245 (Â±0.0234)
RLT RÂ²:       0.8567 (Â±0.0198)
AmÃ©lioration: +3.91%

âœ“ Les combinaisons linÃ©aires amÃ©liorent la performance!
```

---

## ğŸ“ Justification ThÃ©orique

### Pourquoi les Combinaisons LinÃ©aires Fonctionnent?

#### 1. **Augmentation de l'Espace de Splitting**

```
Sans combinaisons:
- Splits basÃ©s sur: X_j â‰¤ t
- LimitÃ© aux axes des coordonnÃ©es

Avec combinaisons:
- Splits basÃ©s sur: Î±â‚X_i + Î±â‚‚X_j â‰¤ t  
- FrontiÃ¨res de dÃ©cision obliques
- Plus flexible et expressif
```

#### 2. **Capture des Interactions**

```python
# Variables sÃ©parÃ©es
Si LSTATâ†‘ â†’ prixâ†“
Si RMâ†‘ â†’ prixâ†‘

# Combinaison
Z = Î±â‚Â·LSTAT + Î±â‚‚Â·RM
Capture l'effet combinÃ© plus prÃ©cisÃ©ment
```

#### 3. **RÃ©duction de DimensionnalitÃ© Intelligente**

```
Au lieu de:
- 13 features individuelles
- Beaucoup de bruit

RLT utilise:
- 3 features importantes
- 6 combinaisons de ces features
- Signal concentrÃ©, moins de bruit
```

---

## ğŸ“ˆ Quand Utiliser les Combinaisons LinÃ©aires?

### âœ… RecommandÃ© Quand:

1. **Haute dimensionnalitÃ©** (p > 20)
   - Beaucoup de features
   - Risque de dilution du signal

2. **Structure Ã©parse** (pâ‚ << p)
   - Peu de variables vraiment importantes
   - Beaucoup de variables bruitÃ©es

3. **CorrÃ©lations entre variables importantes**
   - Les top features interagissent
   - Leurs combinaisons sont informatives

4. **Features continues**
   - Les combinaisons linÃ©aires font sens
   - Pas de catÃ©gorielles pures

### âš ï¸ Ã€ Ã‰viter Quand:

1. **Faible dimensionnalitÃ©** (p < 10)
   - Peu de features originales
   - Combinaisons peuvent sur-ajuster

2. **Variables indÃ©pendantes**
   - Pas d'interactions entre features
   - Combinaisons n'apportent rien

3. **Features catÃ©gorielles**
   - Les combinaisons linÃ©aires perdent du sens
   - PrÃ©fÃ©rer one-hot encoding

---

## ğŸ”¬ DSO1 vs DSO2

### DSO1 (Notre Travail)

**Scope:**
- âœ… Variable Importance (RF + Statistical)
- âœ… Variable Muting
- âœ… **Combinaisons linÃ©aires simples** (expliquÃ©es ici)
- âœ… Baseline vs RLT-RandomForest

**Combinaisons DSO1:**
```python
# Simple weighted averages
Z = 0.5Â·X_i + 0.5Â·X_j

# Poids fixes ou basÃ©s sur VI
```

### DSO2 (Travail Futur)

**Scope:**
- ğŸ”œ **Combinaisons linÃ©aires optimisÃ©es** (recherche de poids)
- ğŸ”œ Interactions d'ordre supÃ©rieur
- ğŸ”œ Feature engineering avancÃ©
- ğŸ”œ Autres modÃ¨les embarquÃ©s (XGBoost, LightGBM)

**Combinaisons DSO2:**
```python
# Optimisation des poids
Z = Î±â‚Â·X_i + Î±â‚‚Â·X_j  oÃ¹ Î± optimisÃ©

# Polynomiales
Z = Î±â‚Â·X_i + Î±â‚‚Â·X_j + Î±â‚ƒÂ·X_iÂ² + Î±â‚„Â·X_iÂ·X_j

# Kernel-based
Z = K(X_i, X_j) fonction noyau
```

---

## ğŸ’¡ Conclusions

### Points ClÃ©s

1. **Les combinaisons linÃ©aires** sont au cÅ“ur de la mÃ©thodologie RLT
2. **Elles permettent** des frontiÃ¨res de dÃ©cision plus flexibles
3. **DSO1** implÃ©mente les combinaisons de base
4. **DSO2** explorera les combinaisons avancÃ©es

### RÃ©sumÃ© de l'Approche

```
RLT Workflow Complet:

1. Calculer VI â†’ Identifier variables importantes
                 
2. Muting      â†’ Ã‰liminer variables faibles
                 
3. Combinaisons â†’ CrÃ©er features composÃ©es
                 
4. EntraÃ®nement â†’ Random Forest sur features meilleures
                 
5. Ã‰valuation  â†’ Comparer avec Baseline
```

---

## ğŸ“š RÃ©fÃ©rences

1. **Zhu, R., Zeng, D., & Kosorok, M. R. (2015).** "Reinforcement Learning Trees." *JASA*
   - Section 2.2: Linear combination splits
   - Section 3.1: Variable importance
   - Section 3.2: Variable muting

2. **Breiman, L. (2001).** "Random Forests." 
   - Baseline methodology

3. **Friedman, J. H. (1991).** "Multivariate adaptive regression splines."
   - Inspiration pour combinaisons linÃ©aires

---

**Authors:** Dhia Romdhane, Yosri Awedi, Baha Saadoui, Nour Rajhi, Bouguerra Taha, Oumaima Nacef  
**Course:** Machine Learning Project - DSO1  
**Repository:** https://github.com/yosriawedi/ML-Project-RLT

---

**Status:** âœ… Documentation ComplÃ¨te - DSO1  
**Next:** DSO2 - Combinaisons OptimisÃ©es et ModÃ¨les AvancÃ©s
