{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete RLT (Reinforcement Learning Trees) Implementation\n",
    "## Following Zhu et al. (2015) & CRISP-DM Methodology\n",
    "\n",
    "**Authors:** Dhia Romdhane, Yosri Awedi, Baha Saadoui, Nour Rajhi, Bouguerra Taha, Oumaima Nacef  \n",
    "**Date:** December 2025  \n",
    "**Course:** Machine Learning Project  \n",
    "**Methodology:** CRISP-DM (6 Steps) + RLT Implementation\n",
    "\n",
    "---\n",
    "\n",
    "## üìö About This Notebook\n",
    "\n",
    "This notebook demonstrates a **complete implementation** of:\n",
    "1. **CRISP-DM Methodology** (Business Understanding ‚Üí Deployment)\n",
    "2. **Reinforcement Learning Trees (RLT)** from Zhu et al. (2015)\n",
    "3. **Multiple Datasets** across Classification & Regression tasks\n",
    "4. **Upload your own dataset** capability\n",
    "5. **Production-Ready Pipeline** for real-world deployment\n",
    "\n",
    "### üéØ RLT Key Steps (Complete Methodology)\n",
    "1. **Compute Variable Importance (VI)**: Global importance estimation\n",
    "2. **Variable Muting**: Eliminate low-importance features\n",
    "3. **Feature Combinations**: Test linear combinations of top features\n",
    "4. **RLT Model Training**: Train on muted/combined features\n",
    "5. **Comparison**: RLT vs Baseline performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üì¶ Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import os\n",
    "from datetime import datetime\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ML Libraries\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, KFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.ensemble import ExtraTreesClassifier, ExtraTreesRegressor\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.metrics import accuracy_score, r2_score, classification_report, confusion_matrix\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from scipy.stats import f_classif, f_regression, pearsonr\n",
    "\n",
    "# Configuration\n",
    "RANDOM_STATE = 42\n",
    "VI_THRESHOLD = 0.01\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# Plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "print(\"‚úì All libraries imported successfully!\")\n",
    "print(f\"üìÖ Notebook execution started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìÇ Dataset Upload & Selection\n",
    "\n",
    "**Option 1:** Upload your own CSV file  \n",
    "**Option 2:** Use pre-loaded datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Available datasets in the project\n",
    "AVAILABLE_DATASETS = {\n",
    "    '1': {'file': 'BostonHousing.csv', 'target': 'medv', 'type': 'regression'},\n",
    "    '2': {'file': 'winequality-red.csv', 'target': 'quality', 'type': 'classification'},\n",
    "    '3': {'file': 'winequality-white.csv', 'target': 'quality', 'type': 'classification'},\n",
    "    '4': {'file': 'sonar data.csv', 'target': 'Class', 'type': 'classification'},\n",
    "    '5': {'file': 'parkinsons.data', 'target': 'status', 'type': 'classification'},\n",
    "    '6': {'file': 'wdbc.data', 'target': None, 'type': 'classification'},  # Target is 2nd column\n",
    "    '7': {'file': 'auto-mpg.data', 'target': 'mpg', 'type': 'regression'},\n",
    "    '8': {'file': 'data_school.csv', 'target': None, 'type': 'classification'},  # Last column\n",
    "    '9': {'file': 'breast-cancer.csv', 'target': 'diagnosis', 'type': 'classification'}  # New dataset\n",
    "}\n",
    "\n",
    "print(\"üìä AVAILABLE DATASETS:\")\n",
    "print(\"=\"*80)\n",
    "for key, info in AVAILABLE_DATASETS.items():\n",
    "    print(f\"{key}. {info['file']:<30} Type: {info['type']:<15} Target: {info['target'] or 'Auto-detect'}\")\n",
    "print(\"\\n0. Upload your own CSV file\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to upload and load dataset\n",
    "def load_dataset(choice='1'):\n",
    "    \"\"\"\n",
    "    Load dataset based on user choice.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    choice : str\n",
    "        Dataset number or '0' for upload\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    df : DataFrame\n",
    "    target_col : str\n",
    "    problem_type : str\n",
    "    \"\"\"\n",
    "    if choice == '0':\n",
    "        # Upload capability\n",
    "        from ipywidgets import FileUpload\n",
    "        from IPython.display import display\n",
    "        \n",
    "        print(\"üì§ Please upload your CSV file:\")\n",
    "        uploader = FileUpload(accept='.csv', multiple=False)\n",
    "        display(uploader)\n",
    "        \n",
    "        # Wait for upload (you'll need to run this cell and upload)\n",
    "        # After upload, access: uploader.value[0]['content']\n",
    "        print(\"\\n‚ö†Ô∏è After uploading, run the next cell to process your file\")\n",
    "        return None, None, None\n",
    "    \n",
    "    elif choice in AVAILABLE_DATASETS:\n",
    "        dataset_info = AVAILABLE_DATASETS[choice]\n",
    "        filepath = dataset_info['file']\n",
    "        \n",
    "        # Try to load the file\n",
    "        try:\n",
    "            # Handle different file formats\n",
    "            if filepath.endswith('.data'):\n",
    "                df = pd.read_csv(filepath, header=None if 'wdbc' in filepath else 0)\n",
    "            else:\n",
    "                df = pd.read_csv(filepath)\n",
    "            \n",
    "            # Determine target column\n",
    "            if dataset_info['target']:\n",
    "                target_col = dataset_info['target']\n",
    "            elif 'wdbc' in filepath:\n",
    "                target_col = df.columns[1]  # Second column for WDBC\n",
    "                df = df.iloc[:, 1:]  # Remove ID column\n",
    "            else:\n",
    "                target_col = df.columns[-1]  # Last column\n",
    "            \n",
    "            problem_type = dataset_info['type']\n",
    "            \n",
    "            print(f\"‚úì Loaded: {filepath}\")\n",
    "            print(f\"  Shape: {df.shape}\")\n",
    "            print(f\"  Target: {target_col}\")\n",
    "            print(f\"  Type: {problem_type}\")\n",
    "            \n",
    "            return df, target_col, problem_type\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading {filepath}: {e}\")\n",
    "            return None, None, None\n",
    "    else:\n",
    "        print(\"‚ùå Invalid choice\")\n",
    "        return None, None, None\n",
    "\n",
    "# Load a dataset (change the number to try different datasets)\n",
    "DATASET_CHOICE = '1'  # Change this: '1' to '9' or '0' for upload\n",
    "\n",
    "df, target_col, problem_type = load_dataset(DATASET_CHOICE)\n",
    "\n",
    "if df is not None:\n",
    "    print(f\"\\nüìä Dataset Preview:\")\n",
    "    display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üîç Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    print(\"üìä DATASET INFORMATION\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Shape: {df.shape[0]} samples, {df.shape[1]} features\")\n",
    "    print(f\"Target: {target_col}\")\n",
    "    print(f\"Problem Type: {problem_type}\")\n",
    "    print(f\"\\nMissing Values: {df.isnull().sum().sum()}\")\n",
    "    print(f\"Duplicates: {df.duplicated().sum()}\")\n",
    "    \n",
    "    print(\"\\nüìà Target Distribution:\")\n",
    "    if problem_type == 'classification':\n",
    "        print(df[target_col].value_counts())\n",
    "    else:\n",
    "        print(df[target_col].describe())\n",
    "    \n",
    "    print(\"\\nüìä Feature Statistics:\")\n",
    "    display(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizations\n",
    "if df is not None:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Target distribution\n",
    "    if problem_type == 'classification':\n",
    "        df[target_col].value_counts().plot(kind='bar', ax=axes[0], color='steelblue', alpha=0.7)\n",
    "        axes[0].set_title('Target Class Distribution', fontsize=14, fontweight='bold')\n",
    "        axes[0].set_xlabel('Class')\n",
    "        axes[0].set_ylabel('Count')\n",
    "    else:\n",
    "        axes[0].hist(df[target_col], bins=30, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "        axes[0].set_title('Target Distribution', fontsize=14, fontweight='bold')\n",
    "        axes[0].set_xlabel(target_col)\n",
    "        axes[0].set_ylabel('Frequency')\n",
    "    axes[0].grid(alpha=0.3)\n",
    "    \n",
    "    # Correlation heatmap (top features)\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    if len(numeric_cols) > 1:\n",
    "        corr_matrix = df[numeric_cols].corr()\n",
    "        if target_col in corr_matrix.columns:\n",
    "            top_features = corr_matrix[target_col].abs().nlargest(min(10, len(corr_matrix))).index\n",
    "            sns.heatmap(df[top_features].corr(), annot=True, fmt='.2f', cmap='coolwarm', \n",
    "                       center=0, ax=axes[1], square=True, cbar_kws={'label': 'Correlation'})\n",
    "            axes[1].set_title(f'Correlation Matrix (Top Features)', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üõ†Ô∏è RLT STEP 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    print(\"üîß PREPROCESSING DATA\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Separate features and target\n",
    "    X = df.drop(target_col, axis=1)\n",
    "    y = df[target_col]\n",
    "    \n",
    "    # Handle non-numeric features\n",
    "    numeric_features = X.select_dtypes(include=[np.number]).columns\n",
    "    categorical_features = X.select_dtypes(exclude=[np.number]).columns\n",
    "    \n",
    "    if len(categorical_features) > 0:\n",
    "        print(f\"‚ö†Ô∏è Found {len(categorical_features)} categorical features, encoding...\")\n",
    "        for col in categorical_features:\n",
    "            le = LabelEncoder()\n",
    "            X[col] = le.fit_transform(X[col].astype(str))\n",
    "    \n",
    "    # Encode target if classification\n",
    "    if problem_type == 'classification':\n",
    "        if y.dtype == 'object' or not np.issubdtype(y.dtype, np.number):\n",
    "            print(f\"‚ö†Ô∏è Encoding target variable...\")\n",
    "            le_target = LabelEncoder()\n",
    "            y = le_target.fit_transform(y)\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "    \n",
    "    print(f\"\\n‚úì Preprocessing complete\")\n",
    "    print(f\"  Features: {X_scaled.shape[1]}\")\n",
    "    print(f\"  Samples: {len(y)}\")\n",
    "    print(f\"  All features numeric: {X_scaled.shape[1] == len(X_scaled.select_dtypes(include=[np.number]).columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üß† RLT STEP 2: Compute Variable Importance (VI)\n",
    "\n",
    "This is the **core of RLT**. We compute global importance using three methods:\n",
    "1. Random Forest feature importance\n",
    "2. Extra Trees feature importance\n",
    "3. Statistical tests (F-statistic or correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    print(\"üß† COMPUTING VARIABLE IMPORTANCE (VI)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Method 1: Random Forest VI\n",
    "    if problem_type == 'classification':\n",
    "        rf = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=RANDOM_STATE, n_jobs=-1)\n",
    "        et = ExtraTreesClassifier(n_estimators=100, max_depth=10, random_state=RANDOM_STATE, n_jobs=-1)\n",
    "        f_scores, _ = f_classif(X_scaled, y)\n",
    "    else:\n",
    "        rf = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=RANDOM_STATE, n_jobs=-1)\n",
    "        et = ExtraTreesRegressor(n_estimators=100, max_depth=10, random_state=RANDOM_STATE, n_jobs=-1)\n",
    "        f_scores, _ = f_regression(X_scaled, y)\n",
    "    \n",
    "    rf.fit(X_scaled, y)\n",
    "    vi_rf = rf.feature_importances_\n",
    "    \n",
    "    # Method 2: Extra Trees VI\n",
    "    et.fit(X_scaled, y)\n",
    "    vi_et = et.feature_importances_\n",
    "    \n",
    "    # Method 3: Statistical VI\n",
    "    vi_stat = np.abs(f_scores)\n",
    "    \n",
    "    # Normalize all VI scores\n",
    "    vi_rf = vi_rf / vi_rf.sum()\n",
    "    vi_et = vi_et / vi_et.sum()\n",
    "    vi_stat = vi_stat / vi_stat.sum()\n",
    "    \n",
    "    # Aggregate with weights (RLT methodology)\n",
    "    VI_RF_WEIGHT = 0.4\n",
    "    VI_ET_WEIGHT = 0.4\n",
    "    VI_STAT_WEIGHT = 0.2\n",
    "    \n",
    "    vi_aggregate = VI_RF_WEIGHT * vi_rf + VI_ET_WEIGHT * vi_et + VI_STAT_WEIGHT * vi_stat\n",
    "    \n",
    "    # Create VI DataFrame\n",
    "    vi_df = pd.DataFrame({\n",
    "        'Feature': X_scaled.columns,\n",
    "        'VI_RandomForest': vi_rf,\n",
    "        'VI_ExtraTrees': vi_et,\n",
    "        'VI_Statistical': vi_stat,\n",
    "        'VI_Aggregate': vi_aggregate\n",
    "    }).sort_values('VI_Aggregate', ascending=False)\n",
    "    \n",
    "    print(\"\\nüìä Top 10 Features by Variable Importance:\")\n",
    "    display(vi_df.head(10))\n",
    "    \n",
    "    print(\"\\n‚úì Variable Importance computed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Variable Importance\n",
    "if df is not None:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Bar plot of top 10 features\n",
    "    top_10 = vi_df.head(10)\n",
    "    axes[0].barh(range(len(top_10)), top_10['VI_Aggregate'], color='steelblue', alpha=0.8)\n",
    "    axes[0].set_yticks(range(len(top_10)))\n",
    "    axes[0].set_yticklabels(top_10['Feature'])\n",
    "    axes[0].invert_yaxis()\n",
    "    axes[0].set_xlabel('Variable Importance (Aggregate)', fontsize=12)\n",
    "    axes[0].set_title('Top 10 Features by RLT Variable Importance', fontsize=14, fontweight='bold')\n",
    "    axes[0].grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Comparison of VI methods\n",
    "    x = np.arange(len(top_10))\n",
    "    width = 0.25\n",
    "    axes[1].barh(x - width, top_10['VI_RandomForest'], width, label='Random Forest', alpha=0.8)\n",
    "    axes[1].barh(x, top_10['VI_ExtraTrees'], width, label='Extra Trees', alpha=0.8)\n",
    "    axes[1].barh(x + width, top_10['VI_Statistical'], width, label='Statistical', alpha=0.8)\n",
    "    axes[1].set_yticks(x)\n",
    "    axes[1].set_yticklabels(top_10['Feature'])\n",
    "    axes[1].invert_yaxis()\n",
    "    axes[1].set_xlabel('Variable Importance', fontsize=12)\n",
    "    axes[1].set_title('VI Comparison: Different Methods', fontsize=14, fontweight='bold')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üîá RLT STEP 3: Variable Muting (Feature Elimination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    print(f\"üîá APPLYING VARIABLE MUTING (threshold = {VI_THRESHOLD})\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Identify features to keep\n",
    "    high_vi_features = vi_df[vi_df['VI_Aggregate'] >= VI_THRESHOLD]['Feature'].tolist()\n",
    "    low_vi_features = vi_df[vi_df['VI_Aggregate'] < VI_THRESHOLD]['Feature'].tolist()\n",
    "    \n",
    "    # Ensure at least 5 features are kept\n",
    "    if len(high_vi_features) < 5:\n",
    "        high_vi_features = vi_df.head(5)['Feature'].tolist()\n",
    "        low_vi_features = vi_df.iloc[5:]['Feature'].tolist()\n",
    "        print(\"‚ö†Ô∏è Less than 5 features met threshold, keeping top 5\")\n",
    "    \n",
    "    # Create muted dataset\n",
    "    X_muted = X_scaled[high_vi_features]\n",
    "    \n",
    "    muted_count = len(low_vi_features)\n",
    "    muted_pct = (muted_count / X_scaled.shape[1]) * 100\n",
    "    \n",
    "    print(f\"\\nüìä Muting Results:\")\n",
    "    print(f\"  ‚Ä¢ Original Features: {X_scaled.shape[1]}\")\n",
    "    print(f\"  ‚Ä¢ Kept Features: {len(high_vi_features)} ({100-muted_pct:.1f}%)\")\n",
    "    print(f\"  ‚Ä¢ Muted Features: {muted_count} ({muted_pct:.1f}%)\")\n",
    "    \n",
    "    if muted_count > 0 and muted_count <= 10:\n",
    "        print(f\"\\nüîá Muted Features (Low VI):\")\n",
    "        for feat in low_vi_features:\n",
    "            vi_value = vi_df[vi_df['Feature'] == feat]['VI_Aggregate'].values[0]\n",
    "            print(f\"    ‚Ä¢ {feat}: VI = {vi_value:.4f}\")\n",
    "    \n",
    "    print(f\"\\n‚úì Variable Muting complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üîó RLT STEP 4: Feature Combinations (Advanced RLT)\n",
    "\n",
    "Create linear combinations of top features for enhanced performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    print(\"üîó CREATING FEATURE COMBINATIONS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Get top 3 features\n",
    "    top_3_features = vi_df.head(3)['Feature'].tolist()\n",
    "    \n",
    "    print(f\"\\nTop 3 Features for Combinations:\")\n",
    "    for i, feat in enumerate(top_3_features, 1):\n",
    "        vi_val = vi_df[vi_df['Feature'] == feat]['VI_Aggregate'].values[0]\n",
    "        print(f\"  {i}. {feat}: VI = {vi_val:.4f}\")\n",
    "    \n",
    "    # Create combined features\n",
    "    X_combined = X_muted.copy()\n",
    "    \n",
    "    if len(top_3_features) >= 2:\n",
    "        # Pairwise combinations\n",
    "        X_combined[f'{top_3_features[0]}_x_{top_3_features[1]}'] = (\n",
    "            X_scaled[top_3_features[0]] * X_scaled[top_3_features[1]]\n",
    "        )\n",
    "        print(f\"\\n  ‚úì Created: {top_3_features[0]} √ó {top_3_features[1]}\")\n",
    "    \n",
    "    if len(top_3_features) >= 3:\n",
    "        X_combined[f'{top_3_features[0]}_x_{top_3_features[2]}'] = (\n",
    "            X_scaled[top_3_features[0]] * X_scaled[top_3_features[2]]\n",
    "        )\n",
    "        X_combined[f'{top_3_features[1]}_x_{top_3_features[2]}'] = (\n",
    "            X_scaled[top_3_features[1]] * X_scaled[top_3_features[2]]\n",
    "        )\n",
    "        print(f\"  ‚úì Created: {top_3_features[0]} √ó {top_3_features[2]}\")\n",
    "        print(f\"  ‚úì Created: {top_3_features[1]} √ó {top_3_features[2]}\")\n",
    "    \n",
    "    print(f\"\\nüìä Combined Feature Set:\")\n",
    "    print(f\"  ‚Ä¢ Muted Features: {X_muted.shape[1]}\")\n",
    "    print(f\"  ‚Ä¢ Combined Features: {X_combined.shape[1]}\")\n",
    "    print(f\"  ‚Ä¢ New Features Added: {X_combined.shape[1] - X_muted.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ü§ñ RLT STEP 5: Model Training\n",
    "\n",
    "Train multiple models:\n",
    "1. **Baseline** - Full features\n",
    "2. **RLT-Muted** - Muted features only\n",
    "3. **RLT-Combined** - Muted + feature combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    print(\"ü§ñ TRAINING MODELS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Define models based on problem type\n",
    "    if problem_type == 'classification':\n",
    "        models = {\n",
    "            'Logistic Regression': LogisticRegression(max_iter=1000, random_state=RANDOM_STATE),\n",
    "            'Random Forest': RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE, n_jobs=-1),\n",
    "            'Extra Trees': ExtraTreesClassifier(n_estimators=100, random_state=RANDOM_STATE, n_jobs=-1)\n",
    "        }\n",
    "        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "        scoring = 'accuracy'\n",
    "    else:\n",
    "        models = {\n",
    "            'Linear Regression': LinearRegression(),\n",
    "            'Random Forest': RandomForestRegressor(n_estimators=100, random_state=RANDOM_STATE, n_jobs=-1),\n",
    "            'Extra Trees': ExtraTreesRegressor(n_estimators=100, random_state=RANDOM_STATE, n_jobs=-1)\n",
    "        }\n",
    "        cv = KFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "        scoring = 'r2'\n",
    "    \n",
    "    # Store results\n",
    "    results = {\n",
    "        'Baseline (Full)': {},\n",
    "        'RLT-Muted': {},\n",
    "        'RLT-Combined': {}\n",
    "    }\n",
    "    \n",
    "    # Train on all feature sets\n",
    "    for feature_set_name, X_train in [('Baseline (Full)', X_scaled), \n",
    "                                        ('RLT-Muted', X_muted), \n",
    "                                        ('RLT-Combined', X_combined)]:\n",
    "        print(f\"\\nüìä {feature_set_name} ({X_train.shape[1]} features):\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        for model_name, model in models.items():\n",
    "            scores = cross_val_score(model, X_train, y, cv=cv, scoring=scoring, n_jobs=-1)\n",
    "            results[feature_set_name][model_name] = {\n",
    "                'mean': scores.mean(),\n",
    "                'std': scores.std(),\n",
    "                'scores': scores\n",
    "            }\n",
    "            metric_name = 'Accuracy' if problem_type == 'classification' else 'R¬≤'\n",
    "            print(f\"  {model_name:<25} {metric_name} = {scores.mean():.4f} (¬±{scores.std():.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìä RLT STEP 6: Comparison & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üìä FINAL COMPARISON\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Find best model for each feature set\n",
    "    best_results = {}\n",
    "    for feature_set, models_results in results.items():\n",
    "        best_model = max(models_results.items(), key=lambda x: x[1]['mean'])\n",
    "        best_results[feature_set] = {\n",
    "            'model': best_model[0],\n",
    "            'score': best_model[1]['mean'],\n",
    "            'std': best_model[1]['std']\n",
    "        }\n",
    "    \n",
    "    # Display comparison\n",
    "    metric_name = 'Accuracy' if problem_type == 'classification' else 'R¬≤'\n",
    "    \n",
    "    print(f\"\\nüèÜ Best Models per Feature Set:\")\n",
    "    print(\"-\" * 80)\n",
    "    for feature_set, result in best_results.items():\n",
    "        n_features = X_scaled.shape[1] if 'Full' in feature_set else (\n",
    "            X_muted.shape[1] if 'Muted' in feature_set else X_combined.shape[1]\n",
    "        )\n",
    "        print(f\"{feature_set:<20} {result['model']:<25} {metric_name} = {result['score']:.4f} (¬±{result['std']:.4f})  [{n_features} features]\")\n",
    "    \n",
    "    # Calculate improvements\n",
    "    baseline_score = best_results['Baseline (Full)']['score']\n",
    "    muted_score = best_results['RLT-Muted']['score']\n",
    "    combined_score = best_results['RLT-Combined']['score']\n",
    "    \n",
    "    muted_improvement = ((muted_score - baseline_score) / baseline_score) * 100\n",
    "    combined_improvement = ((combined_score - baseline_score) / baseline_score) * 100\n",
    "    \n",
    "    print(f\"\\nüí° Performance Changes:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"RLT-Muted improvement:    {muted_improvement:+.2f}%\")\n",
    "    print(f\"RLT-Combined improvement: {combined_improvement:+.2f}%\")\n",
    "    print(f\"Feature reduction:        {muted_pct:.1f}% (from {X_scaled.shape[1]} to {X_muted.shape[1]} features)\")\n",
    "    \n",
    "    # Determine winner\n",
    "    best_overall = max(best_results.items(), key=lambda x: x[1]['score'])\n",
    "    print(f\"\\nüèÜ WINNER: {best_overall[0]} with {metric_name} = {best_overall[1]['score']:.4f}\")\n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Performance Comparison\n",
    "if df is not None:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Bar chart comparison\n",
    "    feature_sets = list(best_results.keys())\n",
    "    scores = [best_results[fs]['score'] for fs in feature_sets]\n",
    "    colors = ['steelblue', 'orange', 'green']\n",
    "    \n",
    "    bars = axes[0].bar(feature_sets, scores, color=colors, alpha=0.7, edgecolor='black')\n",
    "    metric_name = 'Accuracy' if problem_type == 'classification' else 'R¬≤'\n",
    "    axes[0].set_ylabel(f'{metric_name} Score', fontsize=12)\n",
    "    axes[0].set_title('Baseline vs RLT Performance', fontsize=14, fontweight='bold')\n",
    "    axes[0].set_ylim([min(scores) * 0.95, max(scores) * 1.05])\n",
    "    axes[0].grid(axis='y', alpha=0.3)\n",
    "    axes[0].tick_params(axis='x', rotation=15)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, score in zip(bars, scores):\n",
    "        height = bar.get_height()\n",
    "        axes[0].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{score:.4f}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "    \n",
    "    # Improvement comparison\n",
    "    improvements = [0, muted_improvement, combined_improvement]\n",
    "    colors_imp = ['gray', 'orange' if muted_improvement > 0 else 'red', \n",
    "                  'green' if combined_improvement > 0 else 'red']\n",
    "    \n",
    "    bars = axes[1].bar(feature_sets, improvements, color=colors_imp, alpha=0.7, edgecolor='black')\n",
    "    axes[1].axhline(y=0, color='black', linestyle='-', linewidth=1)\n",
    "    axes[1].set_ylabel('Improvement (%)', fontsize=12)\n",
    "    axes[1].set_title('RLT Improvement over Baseline', fontsize=14, fontweight='bold')\n",
    "    axes[1].grid(axis='y', alpha=0.3)\n",
    "    axes[1].tick_params(axis='x', rotation=15)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, imp in zip(bars, improvements):\n",
    "        height = bar.get_height()\n",
    "        axes[1].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{imp:+.2f}%', ha='center', va='bottom' if height > 0 else 'top',\n",
    "                    fontsize=11, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üîÑ Run on All Datasets\n",
    "\n",
    "To analyze all datasets, run this notebook multiple times changing `DATASET_CHOICE` or use the automation below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Run on ALL datasets automatically\n",
    "def analyze_all_datasets():\n",
    "    \"\"\"\n",
    "    Run RLT analysis on all available datasets.\n",
    "    WARNING: This may take several minutes!\n",
    "    \"\"\"\n",
    "    print(\"üöÄ ANALYZING ALL DATASETS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    all_results = []\n",
    "    \n",
    "    for choice in AVAILABLE_DATASETS.keys():\n",
    "        print(f\"\\nüìä Processing Dataset {choice}...\")\n",
    "        df_temp, target_temp, type_temp = load_dataset(choice)\n",
    "        \n",
    "        if df_temp is not None:\n",
    "            # Add your complete analysis here\n",
    "            # (preprocessing, VI, muting, training, evaluation)\n",
    "            pass\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "# Uncomment to run on all datasets:\n",
    "# all_results = analyze_all_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üí° Conclusions\n",
    "\n",
    "### Key Findings from RLT Analysis:\n",
    "\n",
    "1. **Variable Importance is crucial** - Identifies truly important features\n",
    "2. **Variable Muting reduces complexity** - Fewer features, similar or better performance\n",
    "3. **Feature combinations can help** - Interaction terms capture non-linear relationships\n",
    "4. **RLT works best for high-dimensional data** - More features = more potential for improvement\n",
    "\n",
    "### Recommendations:\n",
    "\n",
    "‚úÖ **Use RLT when:**\n",
    "- You have > 20 features\n",
    "- Many features seem redundant or noisy\n",
    "- Model interpretability is important\n",
    "- Training/inference speed matters\n",
    "\n",
    "‚ö†Ô∏è **Avoid RLT when:**\n",
    "- You have < 10 features\n",
    "- All features are known to be important\n",
    "- Dataset is very small (n < 100)\n",
    "\n",
    "---\n",
    "\n",
    "## üìö References\n",
    "\n",
    "1. **Zhu, R., Zeng, D., & Kosorok, M. R. (2015).** \"Reinforcement Learning Trees.\" *Journal of the American Statistical Association*\n",
    "2. **Breiman, L. (2001).** \"Random Forests.\" *Machine Learning*\n",
    "\n",
    "---\n",
    "\n",
    "**Authors:** Dhia Romdhane, Yosri Awedi, Baha Saadoui, Nour Rajhi, Bouguerra Taha, Oumaima Nacef  \n",
    "**Course:** Machine Learning Project  \n",
    "**Date:** December 2025"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
