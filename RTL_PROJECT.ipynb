{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Business Understading**"
      ],
      "metadata": {
        "id": "Y9rc1QdlcWXT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# . Problem Statement\n",
        "\n",
        "many real-world applications, datasets contain a very large number of variables (p can reach hundreds or even thousands), while the number of available observations remains relatively small. In such situations, only a small subset of variables actually carries meaningful information for predicting the target, whereas the remaining variables act merely as noise.\n",
        "\n",
        "This high-dimensional setting creates several challenges for traditional machine learning models. Methods such as Random Forests, CART, and other standard tree-based algorithms often fail to correctly identify the truly informative variables,  avoid selecting irrelevant features during the splitting process, and  capture complex interactions between variables when these interactions do not manifest through marginal effects.\n",
        "\n",
        "Therefore, the central question is:\n",
        "\n",
        "How can we build a predictive model that remains accurate, stable, and reliable in high-dimensional environments (“large p, small n”), where only a small portion of the variables contains real signal, classical models struggle to detect important features, and crucial interactions may exist without any marginal effect?"
      ],
      "metadata": {
        "id": "FCHfnw7m-Owd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BO1 —\n",
        " **BO1 — Identify what truly matters within the available informatio**\n",
        "\n",
        "Many pieces of information are collected, but only a small part is genuinely useful for understanding or predicting the outcome. The goal is to determine which elements are truly important and which ones do not contribute meaningfully\n",
        "\n",
        "\n",
        "# DSO1\n",
        "\n",
        "Build a model capable of reliably estimating variable importance in order to distinguish strong variables from noise variables, even when p ≫ n.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LSKx4fxj-WGt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BO2 —\n",
        "** Improve the quality of the system’s decision-making process**\n",
        "\n",
        "Ensure that the decision-support tool relies mainly on meaningful and relevant information, instead of being influenced by random or misleading elements, so that the results become more consistent and trustworthy..\n",
        "\n",
        "# DSO2\n",
        "Use optimization mechanisms such as reinforcement learning and variable muting to guide the model’s splits toward the variables that are truly relevant.\n",
        "\n"
      ],
      "metadata": {
        "id": "j6mrV_5Z-aTZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BO3\n",
        "# Ensure that the system remains reliable even with a lot of information but few case”\n",
        "\n",
        "The organization needs a predictive tool that stays stable and dependable, even when there are many different pieces of information per case but only a limited number of cases available.\n",
        "\n",
        "# DSO3\n",
        "\n",
        "Develop a predictive model that remains statistically reliable even when the number of variables is significantly higher than the number of observations.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SmkdfOrA-xYB"
      }
    }
  ]
}