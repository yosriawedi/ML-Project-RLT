{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üå≤ RLT Complete Study (Quick Mode)\n",
        "**Author:** Dhia Romdhane\n",
        "\n",
        "## üìä Objectives\n",
        "\n",
        "### Part 1: Real Dataset Analysis\n",
        "- **Comparison:** RLT (with Feature Engineering) vs Baseline Models (without FE)\n",
        "- **RLT Components:** Variable Importance + Muting + Linear Combinations\n",
        "- **Baseline Models:** RF, RF- ‚àöp, RF-log(p), ET, BART, Lasso, Boosting\n",
        "\n",
        "### Part 2: Simulation Study\n",
        "- **Scenario 1:** Classification, independent covariates (N=100)\n",
        "- **Scenario 2:** Non-linear model, independent (N=100)\n",
        "- **Scenario 3:** Checkerboard, strong correlation (N=300)\n",
        "- **Scenario 4:** Linear model (N=200)\n",
        "- Each with **p = 200, 500, 1000**\n",
        "- **10 repetitions** (quick test mode)\n",
        "- **8 models:** RF, RF- ‚àöp, RF-log(p), ET, BART, Lasso, Boosting, RLT-naive\n",
        "\n",
        "### üïí CPU Time Tracking\n",
        "All experiments include detailed CPU time measurements\n",
        "\n",
        "---\n",
        "\n",
        "‚è∞ **Estimated Runtime (Quick Mode):** \n",
        "- Part 1: ~1-2 min (real data)\n",
        "- Part 2: ~30 sec per scenario √ó 3 dimensions\n",
        "- **Total: ~5-7 min**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install xgboost scikit-learn pandas numpy matplotlib seaborn scipy tabulate -q\nprint('‚úÖ Installation termin√©e!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\nimport numpy as np\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tabulate import tabulate\n\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\nfrom sklearn.ensemble import ExtraTreesClassifier, ExtraTreesRegressor\nfrom sklearn.ensemble import AdaBoostClassifier, GradientBoostingRegressor\nfrom sklearn.linear_model import Lasso, LogisticRegression\nfrom xgboost import XGBClassifier, XGBRegressor\n\nfrom sklearn.metrics import accuracy_score, mean_squared_error\nfrom scipy.stats import f_oneway, pearsonr, norm\n\nfrom google.colab import files\nimport io\nimport time\n\nRANDOM_STATE = 42\nnp.random.seed(RANDOM_STATE)\n\nprint('‚úÖ Imports termin√©s!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*70)\nprint(\"‚öôÔ∏è CONFIGURATION\")\nprint(\"=\"*70)\n\n# General\nTEST_SIZE = 0.2\nN_JOBS = -1\n\n# RLT\nVI_THRESHOLD = 0.01\nVI_ET_WEIGHT = 0.5\nVI_STAT_WEIGHT = 0.5\n\n# Tree models\nTREE_CONFIG = {\n    'n_estimators': 100,\n    'random_state': RANDOM_STATE,\n    'n_jobs': N_JOBS\n}\n\n# Simulations - FAST MODE\nSIM_REPS = 10  # Quick test mode\nTEST_SAMPLES = 1000\nP_VALUES = [200, 500, 1000]\n\nprint(f\"\\n‚úÖ Config: {SIM_REPS} reps, test={TEST_SAMPLES}, p={P_VALUES}\")\nprint(f\"‚è±Ô∏è  Estimated time: ~30 sec per scenario √ó 3 dimensions\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*70)\nprint(\"üß† RLT FUNCTIONS\")\nprint(\"=\"*70)\n\ndef compute_vi(X, y, problem_type):\n    \"\"\"Compute Variable Importance\"\"\"\n    if problem_type == 'classification':\n        et = ExtraTreesClassifier(**TREE_CONFIG)\n    else:\n        et = ExtraTreesRegressor(**TREE_CONFIG)\n    \n    et.fit(X, y)\n    vi_et = et.feature_importances_\n    \n    # Statistical VI\n    vi_stat = np.zeros(X.shape[1])\n    for i in range(X.shape[1]):\n        try:\n            if problem_type == 'classification':\n                groups = [X[:, i][y == c] for c in np.unique(y)]\n                f_stat, _ = f_oneway(*groups)\n                vi_stat[i] = f_stat / 1000.0\n            else:\n                corr, _ = pearsonr(X[:, i], y)\n                vi_stat[i] = abs(corr)\n        except:\n            vi_stat[i] = 0\n    \n    # Normalize and aggregate\n    vi_et = vi_et / vi_et.sum() if vi_et.sum() > 0 else vi_et\n    vi_stat = vi_stat / vi_stat.sum() if vi_stat.sum() > 0 else vi_stat\n    vi_agg = VI_ET_WEIGHT * vi_et + VI_STAT_WEIGHT * vi_stat\n    \n    return vi_agg\n\ndef rlt_muting(X_tr, X_te, y_tr, problem_type, level='moderate'):\n    \"\"\"Apply Variable Muting\"\"\"\n    vi = compute_vi(X_tr, y_tr, problem_type)\n    \n    if level == 'no':\n        threshold = 0.0\n    elif level == 'moderate':\n        threshold = max(VI_THRESHOLD, np.mean(vi))\n    else:  # aggressive\n        threshold = max(VI_THRESHOLD, np.median(vi))\n    \n    selected = np.where(vi >= threshold)[0]\n    if len(selected) < 5:\n        selected = np.argsort(vi)[-5:]\n    \n    return X_tr[:, selected], X_te[:, selected], vi[selected]\n\ndef linear_combinations(X, vi, n_comb=2):\n    \"\"\"Create linear combinations\"\"\"\n    if X.shape[1] < 2:\n        return X\n    \n    top_k = min(10, X.shape[1])\n    top_idx = np.argsort(vi)[-top_k:]\n    \n    X_new = X.copy()\n    added = 0\n    \n    for i in range(min(5, len(top_idx)-1)):\n        for j in range(i+1, min(i+3, len(top_idx))):\n            if added >= n_comb * X.shape[1]:\n                break\n            \n            w1 = vi[i]\n            w2 = vi[j]\n            total = w1 + w2\n            w1_n = w1 / total if total > 0 else 0.5\n            w2_n = w2 / total if total > 0 else 0.5\n            \n            new_feat = w1_n * X[:, top_idx[i]] + w2_n * X[:, top_idx[j]]\n            X_new = np.column_stack([X_new, new_feat])\n            added += 1\n    \n    return X_new\n\nprint(\"‚úÖ Fonctions RLT d√©finies!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*70)\nprint(\"üìÅ PARTIE 1: DATASET R√âEL\")\nprint(\"=\"*70)\nprint(\"\\nüëâ Upload your CSV file (last column = target)\\n\")\n\nuploaded = files.upload()\nfilename = list(uploaded.keys())[0]\n\ndf = pd.read_csv(io.BytesIO(uploaded[filename]))\nprint(f\"\\n‚úÖ Loaded: {filename}\")\nprint(f\"   Shape: {df.shape}\")\nprint(f\"   Features: {df.shape[1]-1}\")\n\n# Detect problem type\ntarget_col = df.columns[-1]\nunique_vals = df[target_col].nunique()\n\nif df[target_col].dtype == 'object' or unique_vals < 10:\n    prob_type = 'classification'\n    print(f\"   Type: CLASSIFICATION ({unique_vals} classes)\")\nelse:\n    prob_type = 'regression'\n    print(f\"   Type: REGRESSION\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*70)\nprint(\"üîß PREPROCESSING\")\nprint(\"=\"*70)\n\n# Clean\ndf_clean = df.drop_duplicates()\nfor col in df_clean.columns:\n    if df_clean[col].isnull().sum() > 0:\n        if df_clean[col].dtype in [np.float64, np.int64]:\n            df_clean[col].fillna(df_clean[col].median(), inplace=True)\n        else:\n            df_clean[col].fillna(df_clean[col].mode()[0], inplace=True)\n\n# Separate\nX = df_clean.iloc[:, :-1]\ny = df_clean.iloc[:, -1]\n\n# Encode categorical\ncat_cols = X.select_dtypes(include=['object']).columns\nif len(cat_cols) > 0:\n    X = pd.get_dummies(X, columns=cat_cols, drop_first=True)\n\n# Encode target\nif prob_type == 'classification':\n    if y.dtype == 'object':\n        le = LabelEncoder()\n        y = le.fit_transform(y)\n    else:\n        y = y.values\nelse:\n    y = y.values\n\n# Scale\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Split\nif prob_type == 'classification' and len(np.unique(y)) > 1:\n    try:\n        X_train, X_test, y_train, y_test = train_test_split(\n            X_scaled, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y\n        )\n    except:\n        X_train, X_test, y_train, y_test = train_test_split(\n            X_scaled, y, test_size=TEST_SIZE, random_state=RANDOM_STATE\n        )\nelse:\n    X_train, X_test, y_train, y_test = train_test_split(\n        X_scaled, y, test_size=TEST_SIZE, random_state=RANDOM_STATE\n    )\n\nprint(f\"\\n‚úÖ Ready: Train={X_train.shape[0]}, Test={X_test.shape[0]}, Features={X_train.shape[1]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*70)\nprint(\"üìä COMPARAISON: RLT (avec FE) vs BASELINE (sans FE)\")\nprint(\"=\"*70)\n\nresults_real = []\n\n# Define baseline models\nif prob_type == 'classification':\n    baseline_models = {\n        'RF': RandomForestClassifier(**TREE_CONFIG),\n        'RF-‚àöp': RandomForestClassifier(**{**TREE_CONFIG, 'max_features': max(1, int(np.sqrt(X_train.shape[1])))}),\n        'RF-log(p)': RandomForestClassifier(**{**TREE_CONFIG, 'max_features': max(1, int(np.log(X_train.shape[1])))}),\n        'ET': ExtraTreesClassifier(**TREE_CONFIG),\n        'BART': AdaBoostClassifier(n_estimators=100, random_state=RANDOM_STATE),\n        'Lasso': LogisticRegression(penalty='l1', solver='liblinear', C=10, random_state=RANDOM_STATE),\n        'Boosting': XGBClassifier(n_estimators=100, random_state=RANDOM_STATE, verbosity=0),\n    }\nelse:\n    baseline_models = {\n        'RF': RandomForestRegressor(**TREE_CONFIG),\n        'RF-‚àöp': RandomForestRegressor(**{**TREE_CONFIG, 'max_features': max(1, int(np.sqrt(X_train.shape[1])))}),\n        'RF-log(p)': RandomForestRegressor(**{**TREE_CONFIG, 'max_features': max(1, int(np.log(X_train.shape[1])))}),\n        'ET': ExtraTreesRegressor(**TREE_CONFIG),\n        'BART': GradientBoostingRegressor(n_estimators=100, random_state=RANDOM_STATE),\n        'Lasso': Lasso(alpha=0.1, random_state=RANDOM_STATE),\n        'Boosting': XGBRegressor(n_estimators=100, random_state=RANDOM_STATE, verbosity=0),\n    }\n\n# Test baseline\nprint(\"\\nüîµ BASELINE (sans Feature Engineering):\")\nfor name, model in baseline_models.items():\n    t0 = time.time()\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    cpu = time.time() - t0\n    \n    if prob_type == 'classification':\n        score = accuracy_score(y_test, y_pred)\n        metric = 'Accuracy'\n    else:\n        score = mean_squared_error(y_test, y_pred)\n        metric = 'MSE'\n    \n    results_real.append({\n        'Model': name,\n        'Type': 'Baseline',\n        'Features': X_train.shape[1],\n        metric: score,\n        'CPU(s)': cpu\n    })\n    print(f\"   {name:12s}: {metric}={score:.4f}, CPU={cpu:.3f}s\")\n\n# Test RLT variants\nprint(\"\\nüü¢ RLT (avec Feature Engineering: VI + Muting + Linear Combinations):\")\nfor muting in ['no', 'moderate', 'aggressive']:\n    for n_comb in [1, 2, 5]:\n        t0 = time.time()\n        \n        X_tr_m, X_te_m, vi = rlt_muting(X_train, X_test, y_train, prob_type, muting)\n        X_tr_rlt = linear_combinations(X_tr_m, vi, n_comb)\n        X_te_rlt = linear_combinations(X_te_m, vi, n_comb)\n        \n        if prob_type == 'classification':\n            model = ExtraTreesClassifier(**TREE_CONFIG)\n        else:\n            model = ExtraTreesRegressor(**TREE_CONFIG)\n        \n        model.fit(X_tr_rlt, y_train)\n        y_pred = model.predict(X_te_rlt)\n        cpu = time.time() - t0\n        \n        if prob_type == 'classification':\n            score = accuracy_score(y_test, y_pred)\n        else:\n            score = mean_squared_error(y_test, y_pred)\n        \n        results_real.append({\n            'Model': f'RLT-{muting.capitalize()}-LC{n_comb}',\n            'Type': 'RLT',\n            'Features': X_tr_rlt.shape[1],\n            metric: score,\n            'CPU(s)': cpu\n        })\n        print(f\"   RLT-{muting.capitalize()}-LC{n_comb}: {metric}={score:.4f}, CPU={cpu:.3f}s, Feat={X_tr_rlt.shape[1]}\")\n\n# Display results\nprint(\"\\nüìã TABLEAU COMPLET:\")\ndf_res = pd.DataFrame(results_real)\ndisplay(df_res)\n\n# Best models\ndf_baseline = df_res[df_res['Type'] == 'Baseline']\ndf_rlt = df_res[df_res['Type'] == 'RLT']\n\nascending = (prob_type != 'classification')\nbest_base = df_baseline.sort_values(metric, ascending=ascending).iloc[0]\nbest_rlt = df_rlt.sort_values(metric, ascending=ascending).iloc[0]\n\nprint(f\"\\nüèÜ MEILLEUR BASELINE: {best_base['Model']} ({metric}={best_base[metric]:.4f})\")\nprint(f\"üèÜ MEILLEUR RLT: {best_rlt['Model']} ({metric}={best_rlt[metric]:.4f})\")\n\nif prob_type == 'classification':\n    imp = ((best_rlt[metric] - best_base[metric]) / best_base[metric]) * 100\n    print(f\"üìà Am√©lioration RLT: {imp:+.2f}%\")\nelse:\n    imp = ((best_base[metric] - best_rlt[metric]) / best_base[metric]) * 100\n    print(f\"üìà R√©duction MSE: {imp:+.2f}%\")\n\nprint(\"\\n‚úÖ Partie 1 termin√©e!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*70)\nprint(\"üìä PARTIE 2: SIMULATIONS (Paper RLT - Zhu et al. 2015)\")\nprint(\"=\"*70)\nprint(f\"\\nüî¨ 4 Scenarios √ó 3 Dimensions (p={P_VALUES})\")\nprint(f\"   Reps: {SIM_REPS}, Test samples: {TEST_SAMPLES}\")\nprint(\"\\nCela prendra ~15-20 minutes...\")\n\nsim_results = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*70)\nprint(\"üß™ SCENARIO 1: Classification, Independent Covariates\")\nprint(\"=\"*70)\n\nsim_results['Scenario 1'] = {}\n\nfor p in P_VALUES:\n    print(f\"\\nüìä p={p}...\")\n    \n    errors = {'RF': [], 'RF- ‚àöp': [], 'RF-log(p)': [], 'ET': [], \n              'BART': [], 'Lasso': [], 'Boosting': [], 'RLT-naive': []}\n    \n    for rep in range(SIM_REPS):\n        if rep % 50 == 0:\n            print(f\"   Rep {rep}/{SIM_REPS}...\")\n        \n        # Generate data\n        N = 100\n        X_tr = np.random.uniform(0, 1, (N, p))\n        mu = norm.cdf(10 * (X_tr[:, 0] - 1) + 20 * np.abs(X_tr[:, 1] - 0.5))\n        y_tr = np.random.binomial(1, mu)\n        \n        X_te = np.random.uniform(0, 1, (TEST_SAMPLES, p))\n        mu_te = norm.cdf(10 * (X_te[:, 0] - 1) + 20 * np.abs(X_te[:, 1] - 0.5))\n        y_te = np.random.binomial(1, mu_te)\n        \n        # Baseline models\n        models_base = {\n            'RF': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),\n            'RF- ‚àöp': RandomForestClassifier(n_estimators=100, max_features=max(1, int(np.sqrt(p))), random_state=42, n_jobs=-1),\n            'RF-log(p)': RandomForestClassifier(n_estimators=100, max_features=max(1, int(np.log(p))), random_state=42, n_jobs=-1),\n            'ET': ExtraTreesClassifier(n_estimators=100, random_state=42, n_jobs=-1),\n            'BART': AdaBoostClassifier(n_estimators=100, random_state=42),\n            'Lasso': LogisticRegression(penalty='l1', solver='liblinear', C=10, random_state=42),\n            'Boosting': XGBClassifier(n_estimators=100, random_state=42, verbosity=0),\n            'RLT-naive': ExtraTreesClassifier(n_estimators=100, random_state=42, n_jobs=-1),\n        }\n        \n        for name, model in models_base.items():\n            model.fit(X_tr, y_tr)\n            y_pred = model.predict(X_te)\n            err = 1 - accuracy_score(y_te, y_pred)\n            errors[name].append(err)\n        \n    # Store results\n    sim_results['Scenario 1'][p] = {name: np.mean(errs) for name, errs in errors.items()}\n    sim_results['Scenario 1'][f'{p}_std'] = {name: np.std(errs) for name, errs in errors.items()}\n    \n    print(f\"   ‚úÖ Done! Best: {min(errors.items(), key=lambda x: np.mean(x[1]))[0]}\")\n\nprint(\"\\n‚úÖ Scenario 1 termin√©!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*70)\nprint(\"üß™ SCENARIO 2: Non-linear Model, Independent Covariates\")\nprint(\"=\"*70)\n\nsim_results['Scenario 2'] = {}\n\nfor p in P_VALUES:\n    print(f\"\\nüìä p={p}...\")\n    \n    errors = {'RF': [], 'RF- ‚àöp': [], 'RF-log(p)': [], 'ET': [], \n              'BART': [], 'Lasso': [], 'Boosting': [], 'RLT-naive': []}\n    \n    for rep in range(SIM_REPS):\n        if rep % 50 == 0:\n            print(f\"   Rep {rep}/{SIM_REPS}...\")\n        \n        # Generate data: Y = 100(X1-0.5)^2(X2-0.25)_+ + epsilon\n        N = 100\n        X_tr = np.random.uniform(0, 1, (N, p))\n        y_tr = 100 * (X_tr[:, 0] - 0.5)**2 * np.maximum(X_tr[:, 1] - 0.25, 0) + np.random.normal(0, 1, N)\n        \n        X_te = np.random.uniform(0, 1, (TEST_SAMPLES, p))\n        y_te = 100 * (X_te[:, 0] - 0.5)**2 * np.maximum(X_te[:, 1] - 0.25, 0) + np.random.normal(0, 1, TEST_SAMPLES)\n        \n        # Baseline models\n        models_base = {\n            'RF': RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1),\n            'RF- ‚àöp': RandomForestRegressor(n_estimators=100, max_features=max(1, int(np.sqrt(p))), random_state=42, n_jobs=-1),\n            'RF-log(p)': RandomForestRegressor(n_estimators=100, max_features=max(1, int(np.log(p))), random_state=42, n_jobs=-1),\n            'ET': ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=-1),\n            'BART': GradientBoostingRegressor(n_estimators=100, random_state=42),\n            'Lasso': Lasso(alpha=0.1, random_state=42),\n            'Boosting': XGBRegressor(n_estimators=100, random_state=42, verbosity=0),\n            'RLT-naive': ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=-1),\n        }\n        \n        for name, model in models_base.items():\n            model.fit(X_tr, y_tr)\n            y_pred = model.predict(X_te)\n            mse = mean_squared_error(y_te, y_pred)\n            errors[name].append(mse)\n        \n    sim_results['Scenario 2'][p] = {name: np.mean(errs) for name, errs in errors.items()}\n    sim_results['Scenario 2'][f'{p}_std'] = {name: np.std(errs) for name, errs in errors.items()}\n    \n    print(f\"   ‚úÖ Done! Best: {min(errors.items(), key=lambda x: np.mean(x[1]))[0]}\")\n\nprint(\"\\n‚úÖ Scenario 2 termin√©!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*70)\nprint(\"üß™ SCENARIO 3: Checkerboard Model, Strong Correlation\")\nprint(\"=\"*70)\n\nsim_results['Scenario 3'] = {}\n\nfor p in P_VALUES:\n    print(f\"\\nüìä p={p}...\")\n    \n    errors = {'RF': [], 'RF- ‚àöp': [], 'RF-log(p)': [], 'ET': [], \n              'BART': [], 'Lasso': [], 'Boosting': [], 'RLT-naive': []}\n    \n    # Create correlation matrix\n    Sigma = np.zeros((p, p))\n    for i in range(p):\n        for j in range(p):\n            Sigma[i, j] = 0.9 ** abs(i - j)\n    \n    for rep in range(SIM_REPS):\n        if rep % 50 == 0:\n            print(f\"   Rep {rep}/{SIM_REPS}...\")\n        \n        # Generate data: Y = 2*X50*X100 + 2*X150*X200 + epsilon\n        N = 300\n        X_tr = np.random.multivariate_normal(np.zeros(p), Sigma, N)\n        y_tr = 2 * X_tr[:, 49] * X_tr[:, 99] + 2 * X_tr[:, 149] * X_tr[:, 199] + np.random.normal(0, 1, N)\n        \n        X_te = np.random.multivariate_normal(np.zeros(p), Sigma, TEST_SAMPLES)\n        y_te = 2 * X_te[:, 49] * X_te[:, 99] + 2 * X_te[:, 149] * X_te[:, 199] + np.random.normal(0, 1, TEST_SAMPLES)\n        \n        # Baseline models\n        models_base = {\n            'RF': RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1),\n            'RF- ‚àöp': RandomForestRegressor(n_estimators=100, max_features=max(1, int(np.sqrt(p))), random_state=42, n_jobs=-1),\n            'RF-log(p)': RandomForestRegressor(n_estimators=100, max_features=max(1, int(np.log(p))), random_state=42, n_jobs=-1),\n            'ET': ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=-1),\n            'BART': GradientBoostingRegressor(n_estimators=100, random_state=42),\n            'Lasso': Lasso(alpha=0.1, random_state=42),\n            'Boosting': XGBRegressor(n_estimators=100, random_state=42, verbosity=0),\n            'RLT-naive': ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=-1),\n        }\n        \n        for name, model in models_base.items():\n            model.fit(X_tr, y_tr)\n            y_pred = model.predict(X_te)\n            mse = mean_squared_error(y_te, y_pred)\n            errors[name].append(mse)\n        \n    sim_results['Scenario 3'][p] = {name: np.mean(errs) for name, errs in errors.items()}\n    sim_results['Scenario 3'][f'{p}_std'] = {name: np.std(errs) for name, errs in errors.items()}\n    \n    print(f\"   ‚úÖ Done! Best: {min(errors.items(), key=lambda x: np.mean(x[1]))[0]}\")\n\nprint(\"\\n‚úÖ Scenario 3 termin√©!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*70)\nprint(\"üß™ SCENARIO 4: Linear Model\")\nprint(\"=\"*70)\n\nsim_results['Scenario 4'] = {}\n\nfor p in P_VALUES:\n    print(f\"\\nüìä p={p}...\")\n    \n    errors = {'RF': [], 'RF- ‚àöp': [], 'RF-log(p)': [], 'ET': [], \n              'BART': [], 'Lasso': [], 'Boosting': [], 'RLT-naive': []}\n    \n    # Create correlation matrix\n    Sigma = np.zeros((p, p))\n    for i in range(p):\n        for j in range(p):\n            Sigma[i, j] = 0.5 ** abs(i - j) + 0.2 * (1 if i == j else 0)\n    \n    for rep in range(SIM_REPS):\n        if rep % 50 == 0:\n            print(f\"   Rep {rep}/{SIM_REPS}...\")\n        \n        # Generate data: Y = 2*X50 + 2*X100 + 4*X150 + epsilon\n        N = 200\n        X_tr = np.random.multivariate_normal(np.zeros(p), Sigma, N)\n        y_tr = 2 * X_tr[:, 49] + 2 * X_tr[:, 99] + 4 * X_tr[:, 149] + np.random.normal(0, 1, N)\n        \n        X_te = np.random.multivariate_normal(np.zeros(p), Sigma, TEST_SAMPLES)\n        y_te = 2 * X_te[:, 49] + 2 * X_te[:, 99] + 4 * X_te[:, 149] + np.random.normal(0, 1, TEST_SAMPLES)\n        \n        # Baseline models\n        models_base = {\n            'RF': RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1),\n            'RF- ‚àöp': RandomForestRegressor(n_estimators=100, max_features=max(1, int(np.sqrt(p))), random_state=42, n_jobs=-1),\n            'RF-log(p)': RandomForestRegressor(n_estimators=100, max_features=max(1, int(np.log(p))), random_state=42, n_jobs=-1),\n            'ET': ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=-1),\n            'BART': GradientBoostingRegressor(n_estimators=100, random_state=42),\n            'Lasso': Lasso(alpha=0.1, random_state=42),\n            'Boosting': XGBRegressor(n_estimators=100, random_state=42, verbosity=0),\n            'RLT-naive': ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=-1),\n        }\n        \n        for name, model in models_base.items():\n            model.fit(X_tr, y_tr)\n            y_pred = model.predict(X_te)\n            mse = mean_squared_error(y_te, y_pred)\n            errors[name].append(mse)\n        \n    sim_results['Scenario 4'][p] = {name: np.mean(errs) for name, errs in errors.items()}\n    sim_results['Scenario 4'][f'{p}_std'] = {name: np.std(errs) for name, errs in errors.items()}\n    \n    print(f\"   ‚úÖ Done! Best: {min(errors.items(), key=lambda x: np.mean(x[1]))[0]}\")\n\nprint(\"\\n‚úÖ Scenario 4 termin√©!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*70)\nprint(\"üìä R√âSULTATS DES SIMULATIONS\")\nprint(\"=\"*70)\n\nfor scenario, results in sim_results.items():\n    print(f\"\\n{scenario}:\")\n    print(\"=\"*70)\n    \n    # Create table\n    table_data = []\n    model_names = list(results[P_VALUES[0]].keys())\n    \n    for model in model_names:\n        row = [model]\n        for p in P_VALUES:\n            mean = results[p][model]\n            std = results[f'{p}_std'][model]\n            row.append(f\"{mean:.3f} ({std:.3f})\")\n        table_data.append(row)\n    \n    headers = ['Model'] + [f'p={p}' for p in P_VALUES]\n    print(tabulate(table_data, headers=headers, tablefmt='grid'))\n\nprint(\"\\n‚úÖ √âTUDE COMPL√àTE TERMIN√âE!\")\nprint(\"=\"*70)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "RLT_Complete_Study.ipynb"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}